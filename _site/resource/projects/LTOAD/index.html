<!DOCTYPE html>


<html>
<head>
    <title>Long-tailed Online AD</title>
    <link rel="icon" href="../../favicon.ico" type="image/x-icon">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>    
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/merl.css">
    <link rel="stylesheet" href="./style.css">
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
        });
    </script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"/>
</head>

<body>
  <section class="paper-section">
    <span class="title" style="font-size:36px">Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts</span>
    <br>
    <div class="authors">
      <span style="font-size:25px"><a href="https://ca-joe-yang.github.io/">Chiao-An Yang</a><sup>1</sup></span>
      <span style="font-size:25px"><a href="https://www.merl.com/people/kpeng">Kuan-Chuan Peng</a><sup>2</sup></span>
      <span style="font-size:25px"><a href="https://raymond-yeh.com/">Raymond A. Yeh</a><sup>1</sup></span>
    </div>
    
    <div class="affiliations">
      <span style="font-size:20px"><sup>1</sup>Purdue University, Department of Computer Science</span>
      <br>
      <span style="font-size:20px"><sup>2</sup>Mitsubishi Electric Research Laboratories</span>
    </div>

    <div class="venue">
      <span style="font-size:36px">ICCV 2025</span>
    </div>

    <div class="button-group">
      <a href="https://arxiv.org/abs/2507.16946" target="_blank" class="btn arxiv-btn" aria-label="arXiv Paper">
        <i class="fas fa-file-alt"></i> arXiv
      </a>

      <a href="https://zenodo.org/records/16283853" target="_blank" class="btn zenodo-btn" aria-label="Zenodo Record">
        <i class="fas fa-database"></i> Benchmark
      </a>
    </div>

    <div class="tldr-block">
      <div class="tldr-title">TLDR</div>
      <div class="tldr-content">
        <ol>
          <li>We propose the task and benchmark for long-tailed online AD (<a href="https://zenodo.org/records/16283853">LTOAD</a>);</li>
          <li>We propose a class-agnostic AD framework that does not require class information whatsoever.</li>
        </ol>
      </div>
    </div>
  </section>

  <section class="paper-section">
    <div class="section-title">
      <h1>Abstract</h1>
    </div>
    <p>
      Anomaly detection (AD) identifies the defect regions of a given image. Recent works have studied AD, focusing on
      learning AD without abnormal images, with long-tailed distributed training data, and using a unified model for all
      classes. In addition, online AD learning has also been explored. 
      In this work, we expand in both directions to a realistic setting by considering the novel task of long-tailed
      online AD (LTOAD). We first identified that the offline state-of-the-art LTAD methods cannot be directly applied to the
      online setting. Specifically, LTAD is class-aware, requiring class labels that are not available in the online setting. To
      address this challenge, we propose a class-agnostic framework for LTAD and then adapt it to our online learning set-
      ting. Our method outperforms the SOTA baselines in most offline LTAD settings, including both the industrial manu-
      facturing and the medical domain. In particular, we observe +4.63% image-AUROC on MVTec even compared to
      methods that have access to class labels and the number of classes. In the most challenging long-tailed online setting,
      we achieve +0.53% image-AUROC compared to baselines.
    </p>
    <div class="figure-block">
      <figure class="figure-image-wrapper">
        <img src="assets/teaser.png" alt="Teaser" id="teaser"/>
        <figcaption>
          Figure 1: Comparison of LTOAD to class-aware anomaly detection methods on offline and online learning. 
          (a) Class-aware methods have a class-specific module for each class $c$ in the class
          set $\mathcal{C}$. (b) These methods cannot work in online learning when
          the class labels are unavailable. (c) LTOAD solves this prob-
          lem by learning a concept set $\hat{\mathcal{C}}$ to approximate $\mathcal{C}$. We note that
          $\hat{K} = |\hat{\mathcal{C}}|$ does not need to match $K = |\mathcal{C}|$. (d) In online learning,
          LTOAD can weight input images of seen and unseen classes with
          existing concept-specific modules, i.e., $\{p_\hat{c}\}_{\hat c \in \hat{\mathcal{C}}}$.
        </figcaption>
      </figure>
    </div>
  </section>
  
  <section class="paper-section">
    <div class="section-title">
      <h1>Making LTAD class-agnostic</h1>
    </div>

    <p>
      Given an image $\mathbf{X}$, an AD model $F_\theta$ , parametrized by $\theta$, 
      aims to predict an abnormal map $\widehat{\mathbf{Y}}$ or an abnormal label $\hat{y}$ indicating whether the image is abnormal or not.
    </p>

    $$
    \widehat{\mathbf{Y}}_i = F_\theta(\mathbf{X}_i)
    $$

    <p>
      Class-aware AD methods assume that the class $c$ of the input image is also provided to the model.
    </p>

    $$
    \widehat{\mathbf{Y}}_i = F_\theta(\mathbf{X}_i, c_i)
    $$

    <p>
      To remove the requirement of having $c$, we introduce a concept set $\widehat{\mathcal{C}}$ 
      where we assume that the class information $c$ can be represented as a composition of multiple concepts in $\widehat{\mathcal{C}}$. 
      For example, the class $\it{transistor}$ is related to and derived from concepts $\it{semiconductors}$ and $\it{circuits}$.
      In other words, for each image of class $c$, instead of applying a hard one-hot label, 
      we employ a soft weighting mechanism and assign a soft label $p \in \mathbb{R}^{\hat K}$ where $\hat K = |\widehat{\mathcal{C}}|$.
    </p>

    <p>
      For this approach to be effective, the concept set $\widehat{\mathcal{C}}$ should be representative enough to cover the image classes ${\mathcal{C}}$ of interest.
      Instead of manually selecting the set $\widehat{\mathcal{C}}$, 
      we leverage the zero-shot capability of foundation models 
      where $\widehat{\mathcal{C}}$ is
      learned with only the visual information of the training set and without seeing any class labels.
    </p>

    <div class="figure-block">
      <figure class="figure-image-wrapper">
        <img src="assets/pipeline.png" alt="Pipeline" id="pipeline"/>
        <figcaption>
          Figure 2: Proposed class-agnostic pipeline. 
          We construct concept set $\widehat{\mathcal{C}}$, and the correspondent normal prompts $\mathcal{P}_n$ and abnormal prompts $\mathcal{P}_a$. 
          The concept score $p$ is assigned to each image $\mathbf{X}$ by computing the similarity between $\mathbf{X}$ and each $\hat c \in \widehat{\mathcal{C}}$. 
          It then controls the soft switching mechanism in our class-agnostic reconstruction module $\tt R$ and semantics module $\tt S$. 
          $\tt R$ reconstructs $\mathbf{F}^{\tt i}$ into $\mathbf{F}^{\tt r}$ through Concept VQ and output $\widehat{\mathbf{Y}^{\tt R}}$ 
          by measuring the dissimilarity between $\mathbf{F}^{\tt i}$ and $\mathbf{F}^{\tt r}$.
          S compares the similarity map $\mathbf{S}^{\tt n}$ of $\mathbf{F}^{\tt i}$ and normal prompt features 
          and the similarity map $\mathbf{S}^{\tt a}$ of $\mathbf{F}^{\tt i}$ and abnormal prompt features to output $\widehat{\mathbf{Y}^{\tt S}}$. 
          The final prediction $\widehat{\mathbf{Y}}$ is aggregated from $\widehat{\mathbf{Y}^{\tt R}}$ and $\widehat{\mathbf{Y}^{\tt S}}$.
.
        </figcaption>
      </figure>
    </div>
  </section>

  <section class="paper-section">
    <div class="section-title">
      <h1>LTOAD benchmark</h1>
    </div>

    <p>
      Given a model $F_{\theta_0}$ where $\theta_0$ is the parameters trained offline on an LTAD dataset, 
      the goal of LTOAD is to update the parameters $\theta_t$ in an online manner to improve the performance on a data stream 
      $\widetilde{\mathbf{X}}_{\leq t} = \left[ \widetilde{\mathbf{X}}_1,···, \widetilde{\mathbf{X}}_t \right] $ where each image $\widetilde{\mathbf{X}}$ comes sequentially. 
      Formally, we focus on improving the accuracy of $F_{\theta_t}(\widetilde{\mathbf{X}}_{\leq t}) = \hat{\mathbf{Y}_t}$ where $\hat{\mathbf{Y}}_t$ is the prediction at $t$. 
      Note that $\widetilde{\mathbf{X}}_{\leq t}$ is an ordered list, i.e., LTOAD is evaluated sequentially.
    </p>

    <p>
      We consider the any-$\Delta$ inference setting where the model is updated on small batches of data samples of size $\Delta$. 
      As the data comes in a stream, a model's online learning performance is highly related to the ordering data. 
      To study the effect, we sequentially split the data stream into sessions which corresponds to a sublist of the dataset.
    </p>

    <div class="figure-block">      
      <figure class="figure-image-wrapper">
        <img src="assets/configurations_table.png" alt="Configurations" id="configurations_table"/>
        <figcaption>Table 1. We define 8 configurations with combinations of different session type $\in$ {blurry, disjoint} and ordering type $\in$ {head-first, head-first, else}.</figcaption>
      </figure>
      <figure class="figure-image-wrapper">
        <img src="assets/configurations_figure.png" alt="Configurations" id="configurations_figure"/>
        <figcaption>
          Figure 3. We visualize the 8 configurations with a toy example 
          where $\mathcal{C}_{\text{head}}$ and $\mathcal{C}_{\text{tail}}$ each has 5 classes.
        </figcaption>
      </figure>

    </div>
  </section>


    

  <section class="paper-section">
    <div class="section-title">
      <h1>Experiments</h1>
    </div>

    <p>
      Our class-agnostic LTOAD framework consistently outperform SOTA AD methods on most long-tailed settings. Noted that being class-agnostic is a more challenging setting than not.
    </p>

    <div class="figure-block">
      <figure>
        <img src="assets/quant.png" alt="Quantitative Comparison" id="quant"/>
        <figcaption>
          Table 2: Comparison ($\uparrow$) on long-tailed AD offline <a href="https://www.mvtec.com/company/research/datasets/mvtec-ad">MVTec</a> 
          in image-level AUROC for anomaly detection (Det.) and pixel-level AUROC for anomaly segmentation (Seg.). 
          The column CA (class-agnostic) indicates whether a method requires class names or the number of classes during training or not. 
          We mark the best and second best performances in bold and underline.
        </figcaption>
      </figure>
    </div>

    <div class="figure-block">
      <figure>
        <img src="assets/qual.png" alt="Qualitative Comparison" id="qual"/>
        <figcaption>
          Figure 4: Qualitative comparison among <a href="https://arxiv.org/abs/2403.20236">LTAD</a>, <a href="https://arxiv.org/abs/2310.14228">HVQ</a>, and LTOAD (ours) on MVTec. 
          Inputs from $\mathcal{C}_{\text{head}}$ / $\mathcal{C}_{\text{tail}}$  are outlined in blue / red.
        </figcaption>
      </figure>
    </div>


    <p>
      Our online learning algorithm $\mathcal{A}^{\tt AA}$ improves steadily under both D5-HF and D5-TF 
      while the baseline $\mathcal{A}^{\tt N}$ falls off during later steps.
    </p>

    <div class="figure-block">
      <figure>
        <img src="assets/online.png" alt="Online Performance" id="online"/>
        <figcaption>
          Figure 5: Performance curve of $\mathcal{A}^{\tt N}$ (baseline algorithm) and $\mathcal{A}^{\tt AA}$ (our online learning algorithm) 
          in pixel-level AUROC on $\mathcal{C}_{\text{head}}$ and $\mathcal{C}_{\text{tail}}$.
        </figcaption>
      </figure>
    </div>
    
  </section>

  <section class="paper-section">
    <div class="section-title">
      <h1>Citation</h1>
    </div>
    <div class="bibtex-block"><code> @inproceedings{yang2025ltoad,
    &nbsp;title={Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts},
    &nbsp;author={Yang, Chiao-An and Peng, Kuan-Chuan and Yeh, Raymond A.},
    &nbsp;booktitle={Proc. ICCV},
    &nbsp;year={2025}
  }</code></div>
  </section>

</body>
</html>
