<!DOCTYPE html>


<html>
<head>
    <title>4D-RGPT</title>
    <link rel="icon" href="../../favicon.ico" type="image/x-icon">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/nvidia.css">
    <link rel="stylesheet" href="./style.css">
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
        });
    </script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"/>
    <script>
      document.addEventListener('DOMContentLoaded', function() {
        const items = document.querySelectorAll('.video-carousel-item');
        const dots = document.querySelectorAll('.carousel-dot');
        let currentIndex = 0;

        function showSlide(index) {
          items.forEach((item, i) => {
            item.classList.toggle('active', i === index);
          });
          dots.forEach((dot, i) => {
            dot.classList.toggle('active', i === index);
          });
          currentIndex = index;
        }

        dots.forEach(dot => {
          dot.addEventListener('click', () => {
            showSlide(parseInt(dot.getAttribute('data-index')));
          });
        });
      });
    </script>
</head>

<body>
  <section class="paper-section">
    <span class="title" style="font-size:48px">4D-RGPT:</span>
    <br>
    <span class="title">Toward Region-level 4D Understanding via Perceptual Distillation</span>
    <div class="authors">
      <span><a href="https://ca-joe-yang.github.io/">Chiao-An Yang</a><sup>1,2†</sup></span>
      <span><a href="https://ryohachiuma.github.io/">Ryo Hachiuma</a><sup>2</sup></span>
      <span><a href="https://sifeiliu.net/">Sifei Liu</a><sup>2</sup></span>
      <span><a href="https://research.nvidia.com/labs/lpr/author/subhashree-radhakrishnan/">Subhashree Radhakrishnan</a><sup>2</sup></span>
    </div>
    <div class="authors">
      <span><a href="https://raymond-yeh.com/">Raymond A. Yeh</a><sup>1</sup></span>
      <span><a href="https://vllab.ee.ntu.edu.tw/ycwang.html">Yu-Chiang Frank Wang</a><sup>2</sup></span>
      <span><a href="https://minhungchen.netlify.app/">Min-Hung Chen</a><sup>2</sup></span>
    </div>

    <div class="affiliations">
      <span style="font-size:20px"><sup>1</sup>Purdue University</span>
      <br>
      <span style="font-size:20px"><sup>2</sup>NVIDIA</span>
      <br>
      <span style="font-size:16px"><sup>†</sup>Work done during internship</span>
    </div>

    <!-- <div class="venue">
      <span style="font-size:30px">Under review</span>
    </div> -->

    <div class="button-group">
      <a href="" target="_blank" class="btn arxiv-btn" aria-label="arXiv Paper">
        <i class="fas fa-file-alt"></i> arXiv (Coming Soon)
      </a>

      <a href="." target="_blank" class="btn github-btn" aria-label="GitHub Repository">
        <i class="fab fa-github"></i> Benchmark & Code (Coming Soon)
    </a>
    </div>

    <!-- We propose a new task: region-level 4D VQA (R4D-Bench), which requires MLLM to understand <span class="highlight-green">Region</span>, Depth and Time.
Recent general/3D MLLMs struggle with some or all of these aspects.To address this gap, we propose a training framework to distill 4D knowledge to a general MLLM. -->


    <div class="tldr-block">
      <div class="tldr-title">TLDR</div>
      <div class="tldr-content">
        <ol>
          <li>We propose region-level 4D VQA benchmark <a href="">R4D-Bench</a>, which requires MLLM to understand <span class="highlight-green">region</span>, <span class="highlight-green">depth</span> and <span class="highlight-green">time</span>.</li>
          <li>To address the gap of recent general/3D MLLMs, we propose a training framework to <span class="highlight-green">distill 4D knowledge</span> to a general MLLM.</li>
        </ol>
      </div>
    </div>
  </section>

  <section class="paper-section">
    <div class="section-title">
      <h1>Abstract</h1>
    </div>
    <p>
      Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited,
      constrained by weak 4D perception and temporal understanding.
      Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting.
      We tackle these issues by introducing:
    </p>
    <ul class="custom-list">
      <li><span class="highlight-green">4D-RGPT</span>, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception;</li>
      <li>Perceptual 4D Distillation (<span class="highlight-green">P4D</span>), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception;</li>
      <li><span class="highlight-green">R4D-Bench</span>, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline.</li>
    </ul>
    <p>
      Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.
    </p>
    <div class="figure-block">
      <figure class="figure-image-wrapper" id="teaser-block">
        <img src="assets/teaser.png" alt="Teaser" id="teaser"/>
        <figcaption>
          <strong>Figure 1: Overview of Region-level 4D Understanding.</strong>
          4D region-level VQA, e.g., our R4D-Bench, requires MLLMs to be able to track
          regions (2D), perceive depth (3D), and temporal progression (4D).
          Baseline MLLMs cannot recognize one or more of these aspects and thus
          fail to answer questions correctly. With our distillation framework, our 4D-RGPT better perceives these aspects and answers accurately. We
          note that the regions labeled with (*) are not provided in R4D-Bench; they are visualized for readability.
        </figcaption>
      </figure>
    </div>

    <div class="figure-block">
      <figure>
        <video controls width="100%" style="max-width: 900px;">
          <source src="assets/424.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <figcaption>
          <strong>Video 1: An example of Region-level 4D VQA.</strong>
        </figcaption>
      </figure>
    </div>

  </section>

  <section class="paper-section">
    <div class="section-title">
      <h1>Approach</h1>
    </div>

    <p>
      Given a video $\mathbf{V}$ and a question $Q$, an MLLM responds with an answer $A$ autoregressively.
      We develop a specialized MLLM by incorporating 4D knowledge from a teacher model and leveraging low-level representations, e.g., depth and optical flow, over time.
    </p>
    <p>
      To this end, we design <span class="highlight-green">4D-RGPT</span> to capture both <span class="highlight-purple">latent</span> 4D features and <span class="highlight-purple">explicit</span> 4D signals from V with training-only modules.
      These 4D representations enable the model to better perceive 4D knowledge during training, without introducing additional inference cost.
    </p>
    <p>
      We introduce our Perceptual 4D Distillation (<span class="highlight-green">P4D</span>) framework to distill 4D knowledge into 4D-RGPT during training.
      Our framework leverages a frozen expert 4D perception model (teacher) to supervise both <span class="highlight-purple">latent</span> and <span class="highlight-purple">explicit</span> 4D representations of 4D-RGPT (student).
      The latent distillation provides intermediate guidance on abstract 4D features,
      while the explicit distillation ensures accurate extraction of interpretable low-level 4D signals.
    </p>




    <div class="figure-block">
      <figure class="figure-image-wrapper">
        <img src="assets/pipeline.png" alt="Pipeline" id="pipeline"/>
        <figcaption>
          <strong>Figure 2. Perceptual 4D Distillation (P4D) framework for 4D-RGPT.</strong>
          For each frame $\mathbf{I}^{(i)}$ in $\mathbf{V}$,
          4D-RGPT extracts 4D representations through training-only modules, i.e., $\mathbf{D}_{\texttt{4DP}}$ and $\mathbf{D}_m$ for $m \in \mathcal{M}$.
          This includes both latent features, i.e., $\hat{\mathbf{F}}_{\texttt{4D}}$, and explicit signals, e.g., depth
$\hat{\mathbf{P}}_{depth}$ or optical flow maps $\hat{\mathbf{P}}_{flow}$. We also incorporate timestamp positional encodings (TPE) to provide temporal cues for 4D-RGPT to be
temporally aware. In the P4D framework, the frozen teacher, i.e., 4D perception model, captures 4D expert knowledge from $\mathbf{V}$. It is then
distilled to the student 4D-RGPT via <span class="highlight-purple">latent</span> and <span class="highlight-purple">explicit</span> distillation.
        </figcaption>
      </figure>
    </div>

    <div class="paper-subsection">
      <h2 class="highlight-purple">Latent Distillation (LD)</h2>
      <p>
        We align the latent $\hat{\mathbf{F}}_{\texttt{4D}}$ with the teacher's intermediate features $\mathbf{F}_{\texttt{4D}}$,
      </p>
      $$
      \mathcal{L}_{\texttt{LD}} = \sum_{n'}^{N'} \Delta_{\texttt{4D}}(\hat{\mathbf{F}}^{(n')}_{\texttt{4D}}, \mathbf{F}^{(n')}_{\texttt{4D}}).
      $$
    </div>

    <div class="paper-subsection">
      <h2 class="highlight-purple">Explicit Distillation (ED)</h2>
      <p>
        We align the explicit $\hat{\mathbf{P}}_m$ with the teacher's final signals $\mathbf{P}_m$,
      </p>

      $$
      \mathcal{L}_{\texttt{ED}} = \sum_{m \in \mathcal{M}} \sum_{n}^{N} \Delta_{\texttt{4D}}(\hat{\mathbf{P}}_{(m)}^{(n)}, \mathbf{P}^{(n)}_{m}).
      $$
    </div>
  </section>

  <section class="paper-section">
    <div class="section-title">
      <h1>R4D-Bench</h1>
    </div>

    <p>
      Existing benchmarks do not evaluate MLLMs on 4D region-based understanding in complex, real-world scenarios and lack the following critical properties:
      <ul class="custom-list">
        <li><strong>Lack of Dynamic Scenes</strong>: Most focus on scenes with static objects.</li>
        <li><strong>Lack of Region Prompting</strong>: Region prompts allow controlled and intuitive user queries.</li>
      </ul>
<!-- \begin{itemize}
    \item \textbf{Lack of Dynamic Scenes}:
    Most focus on indoor scenes with minimal object interaction or constrained movement, which do not fully capture the complexity of real-world object manipulation and dynamic changes.
    \item \textbf{Lack of Region Prompting}:
    Region prompts allow controlled and intuitive user queries in VQA.
    Without this ability, an MLLM's interpretability and usability in practical applications are hindered.
\end{itemize} -->

    <p>
      To address these gaps, we introduce <span class="highlight-green">R4D-Bench</span> challenging MLLMs with region-level 4D VQA, where depth and temporal perception are critical.
    </p>

    <p>
      Given an input video $\mathbf{V} = [\mathbf{I}^{(n)}]_{n=1:N}$ of $N$ frames,
      a region-prompted 4D question $\mathbf{Q}$,
      and a set of region masks $\mathbf{M}$ describing the objects of interest in $\mathbf{Q}$ in $\mathbf{I}^{(1)}$,
      the task is to respond with the correct or most suitable answer from a set of options.
    </p>


    <p>
      We curate R4D-Bench based on existing non-region-based 4D VQA benchmarks, i.e., <a href="https://arxiv.org/abs/2503.23765">STI-Bench</a> and <a href="https://arxiv.org/abs/2508.02095">VLM4D</a>.
      Our pipeline employs a hybrid automated and human-verified process to transform conventional VQ pairs into highly specific region-prompted questions.
    </p>

    <div class="figure-block">
      <figure class="figure-image-wrapper">
        <img src="assets/region4d.png" alt="Configurations" id="configurations_table"/>
        <figcaption>
          <strong>Figure 3: Curation pipeline of our R4D-Bench.</strong>
        </figcaption>
      </figure>
    </div>

    <p>
      Given existing non-region 4D VQA benchmarks,
      <ul class="custom-list">
        <li>We first extract the noun keywords from the question as candidates for objects of interest.</li>
        <li>We generate segmentation masks for these objects if ground truth masks are not provided.</li>
        <li>We create a SoM image to represent object regions.</li>
        <li>The SoM images are then used to prompt a VLM to match objects with the regions.</li>
        <li>Finally, we follow up with human verification to ensure accuracy.</li>
      </ul>
    </p>

    <p>
      Our R4D-Bench consists of 1,517 region-prompted VQAs.
      Each question is a multiple-choice problem with four to five answer options.
    </p>
    <p>
      The <span class="highlight-gray">static</span> split (418 VQAs) includes 3 categories:
      Dimension Measurement (<span class="highlight-gray">DM</span>), 3D Video Grounding (<span class="highlight-gray">VG</span>), and Spatial Relation (<span class="highlight-gray">SR</span>).
    </p>
    <p>
      The <span class="highlight-pink">dynamic</span> split (1,098 VQAs) includes 6 categories:
      Counting (<span class="highlight-pink">C</span>), Translational Movement (<span class="highlight-pink">T</span>), Rotational Movement (<span class="highlight-pink">R</span>), False Positive Detection (<span class="highlight-pink">FP</span>), Speed & Acceleration Estimation (<span class="highlight-pink">SA</span>), and Displacement & Path Length Measurement (<span class="highlight-pink">DP</span>).
    </p>

    <div class="video-carousel">
      <div class="video-carousel-container">
        <div class="video-carousel-item active">
          <video controls width="100%">
            <source src="assets/validation_7.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 2: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
        <div class="video-carousel-item">
          <video controls width="100%">
            <source src="assets/549.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 3: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
        <div class="video-carousel-item">
          <video controls width="100%">
            <source src="assets/643.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 4: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
        <div class="video-carousel-item">
          <video controls width="100%">
            <source src="assets/24.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 5: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
        <div class="video-carousel-item">
          <video controls width="100%">
            <source src="assets/validation_204.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 6: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
        <div class="video-carousel-item">
          <video controls width="100%">
            <source src="assets/validation_1313.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 6: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
        <div class="video-carousel-item">
          <video controls width="100%">
            <source src="assets/validation_853.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 7: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
        <div class="video-carousel-item">
          <video controls width="100%">
            <source src="assets/validation_622.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption class="video-caption">
            <strong>Video 8: More examples of R4D-Bench.</strong>
          </figcaption>
        </div>
      </div>
      <div class="video-carousel-nav">
        <button class="carousel-dot active" data-index="0"></button>
        <button class="carousel-dot" data-index="1"></button>
        <button class="carousel-dot" data-index="2"></button>
        <button class="carousel-dot" data-index="3"></button>
        <button class="carousel-dot" data-index="4"></button>
        <button class="carousel-dot" data-index="5"></button>
        <button class="carousel-dot" data-index="6"></button>
        <button class="carousel-dot" data-index="7"></button>
      </div>
    </div>

  </section>




  <section class="paper-section">
    <div class="section-title">
      <h1>Experiments</h1>
    </div>

    <p>
      We compare our 4D-RGPT with various
      proprietary MLLMs, e.g., GPT, Gemini-Pro;
      open-source generalized MLLMs, e.g.,
      <a href="https://arxiv.org/abs/2412.05271">InternVL-2.5</a>,
      <a href="https://arxiv.org/abs/2502.13923">Qwen2.5-VL</a>;
      and recent 3D/4D specialized MLLMs, e.g.,
      <a href="https://arxiv.org/abs/2504.20024">SpatialReasoner</a>,
      <a href="https://arxiv.org/abs/2506.09965">ViLaSR</a>,
      and
      <a href="https://arxiv.org/abs/2504.01805">SpaceR</a>.
    </p>

    <div class="figure-block">
      <figure>
        <img src="assets/non_region_table.png" style="max-width: 70%;"/>
        <figcaption>
          <strong>Table 1: Comparison ($\uparrow$) on existing 4D VQA benchmarks.</strong>
          We report the performance on
          <a href="https://arxiv.org/abs/2503.23765">STI-Bench</a> (STI),
          <a href="https://arxiv.org/abs/2508.02095">VLM4D-real</a> (V4D),
          <a href="https://arxiv.org/pdf/2505.23764">MMSI-Bench</a> (MMSI),
          <a href="https://arxiv.org/abs/2506.03135">OmniSpatial</a> (OS),
          <a href="https://arxiv.org/abs/2412.07755">SAT</a>,
          and
          <a href="https://arxiv.org/abs/2505.20279">VSTI-Bench</a> (VSTI).
        </figcaption>
      </figure>
    </div>

    <div class="figure-block">
      <figure>
        <img src="assets/region_table.png"/>
        <figcaption>
          <strong>Table 2: Comparison ($\uparrow$) on our R4D-Bench in accuracy (%).</strong>
          We report performance on the <span class="highlight-pink">static</span> split, the <span class="highlight-pink">dynamic</span> split, and all 9 tasks of R4D-Bench.
          <!-- Figure 4: Qualitative comparison among <a href="https://arxiv.org/abs/2403.20236">LTAD</a>, <a href="https://arxiv.org/abs/2310.14228">HVQ</a>, and LTOAD (ours) on MVTec.
          Inputs from $\mathcal{C}_{\text{head}}$ / $\mathcal{C}_{\text{tail}}$  are outlined in blue / red. -->
        </figcaption>
      </figure>
    </div>

  </section>
<!--
  <section class="paper-section">
    <div class="section-title">
      <h1>Citation</h1>
    </div>
    <div class="bibtex-block"><code> TBD</code></div>
  </section> -->

</body>
</html>
